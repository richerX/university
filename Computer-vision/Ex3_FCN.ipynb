{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKI5IOhpiYy0"
      },
      "source": [
        "# Load dataset in Pytorch dataset class\n",
        "\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "\n",
        "trainset = datasets.CIFAR10(\"content\", train=True,  download=True)\n",
        "testset = datasets.CIFAR10(\"content\", train = False, download=True)\n",
        "\n",
        "# Display some samples\n",
        "for j in range(10):\n",
        "  image, class_num = trainset[j]\n",
        "  plt.subplot(1, 10 ,j+1)\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.title(str(class_num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lglfSA0je2W"
      },
      "source": [
        "# Validation function. Don't cahnge it\n",
        "def validate(model,testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "  \n",
        "  return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zLprG7kk-Q9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TwoLayerFCN(nn.Module):\n",
        "\n",
        "    size = 32 * 32 * 3\n",
        "\n",
        "    def __init__(self, class_nums = 10):\n",
        "        super(TwoLayerFCN, self).__init__()\n",
        "        # Define two layer fully - connected network with linear layers: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "\n",
        "        # Put your code here\n",
        "        self.layer1 = nn.Linear(self.size, 256)\n",
        "        self.layer2 = nn.Linear(256, class_nums)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Preprocess image, remember that first dimension is batch dimension !\n",
        "\n",
        "        # Perform forward pass;\n",
        "        # Use layers defined in constructor;\n",
        "        # Add activation function on your choice: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "        \n",
        "        # Put your code here\n",
        "        x = x.view(-1, self.size)\n",
        "        scores = F.relu(self.layer1(x))\n",
        "        scores = self.layer2(scores)\n",
        "\n",
        "        # return raw output of last layer\n",
        "        return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-r1AjqwY0It"
      },
      "source": [
        "# Prepare the data\n",
        "transform=transforms.Compose([\n",
        "                              transforms.ToTensor(), # Pil Image to Pytorch tensor\n",
        "                              transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) # https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize\n",
        "                              ])\n",
        "\n",
        "trainset.transform = transform\n",
        "testset.transform = transform\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size = 512, shuffle = True)\n",
        "test_loader = DataLoader(testset, batch_size = 256, shuffle = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcJFI3hTOJlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d7d8fc-d347-4b7c-d141-8a56a8b0db55"
      },
      "source": [
        "# Train the model\n",
        "# You may change loss function, learning rate and number of epochs\n",
        "\n",
        "model = TwoLayerFCN(10) \n",
        "model.train()\n",
        "\n",
        "# By default cross-entropy loss used, you can change it to another loss function\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "# You can change learning rate and number of epoch too\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) \n",
        "for epoch in range(25):\n",
        "    for img_batch, labels_batch in train_loader:\n",
        "\n",
        "        output = model(img_batch)\n",
        "        loss = criterion(output, labels_batch)\n",
        "        # Get the grad and update model weights: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#backprop\n",
        "\n",
        "        # Put your code here\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    accuracy = validate(model,test_loader)\n",
        "    print(\"Epoch {} Loss {:.2f} Accuracy {:.2f}\".format(epoch,loss.item(),accuracy))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss 1.99 Accuracy 0.31\n",
            "Epoch 1 Loss 1.86 Accuracy 0.35\n",
            "Epoch 2 Loss 1.88 Accuracy 0.37\n",
            "Epoch 3 Loss 1.78 Accuracy 0.39\n",
            "Epoch 4 Loss 1.70 Accuracy 0.40\n",
            "Epoch 5 Loss 1.75 Accuracy 0.41\n",
            "Epoch 6 Loss 1.70 Accuracy 0.41\n",
            "Epoch 7 Loss 1.77 Accuracy 0.42\n",
            "Epoch 8 Loss 1.65 Accuracy 0.42\n",
            "Epoch 9 Loss 1.66 Accuracy 0.43\n",
            "Epoch 10 Loss 1.60 Accuracy 0.44\n",
            "Epoch 11 Loss 1.55 Accuracy 0.44\n",
            "Epoch 12 Loss 1.65 Accuracy 0.44\n",
            "Epoch 13 Loss 1.59 Accuracy 0.45\n",
            "Epoch 14 Loss 1.62 Accuracy 0.45\n",
            "Epoch 15 Loss 1.54 Accuracy 0.45\n",
            "Epoch 16 Loss 1.61 Accuracy 0.46\n",
            "Epoch 17 Loss 1.52 Accuracy 0.46\n",
            "Epoch 18 Loss 1.51 Accuracy 0.46\n",
            "Epoch 19 Loss 1.44 Accuracy 0.46\n",
            "Epoch 20 Loss 1.61 Accuracy 0.46\n",
            "Epoch 21 Loss 1.58 Accuracy 0.47\n",
            "Epoch 22 Loss 1.54 Accuracy 0.47\n",
            "Epoch 23 Loss 1.48 Accuracy 0.47\n",
            "Epoch 24 Loss 1.49 Accuracy 0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "v3HoH0A0POIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ideas for extra work\n",
        "\n",
        "* Compare different optimizators \n",
        "* Compare different Activation functions\n",
        "* Evaluate Batch Normalization effect (need a deeper network) "
      ],
      "metadata": {
        "id": "jRCUb1NdPasd"
      }
    }
  ]
}