{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6dWwjPmrePT"
   },
   "source": [
    "# Load the dataset\n",
    "Don't change this code\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XKI5IOhpiYy0",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Load and preprocess the data. Don't change this code\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "# CIFAR10 z-normalization const https://github.com/facebookarchive/fb.resnet.torch/issues/180\n",
    "cifar10_mean = (0.491, 0.482, 0.447)\n",
    "cifar10_std = (0.247, 0.244, 0.262)\n",
    "\n",
    "# Data preprocessing\n",
    "transform=transforms.Compose([\n",
    "                              transforms.ToTensor(), # PIL Image to Pytorch tensor\n",
    "                              transforms.Normalize(cifar10_mean, cifar10_std) # https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20normalize#torchvision.transforms.Normalize\n",
    "                              ])\n",
    "\n",
    "dataset = datasets.CIFAR10(\"content\", train=True, transform = transform ,  download=True)\n",
    "\n",
    "# Load class names\n",
    "with open(\"content/cifar-10-batches-py/batches.meta\",'rb') as infile:\n",
    "  cifar_meta = pickle.load(infile)\n",
    "labels = cifar_meta['label_names']\n",
    "\n",
    "# Split dataset into train and val\n",
    "train_ds, val_ds, _= random_split(dataset, [10000, 2000 ,38000])\n",
    "batch_size = 256\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
    "val_loader = DataLoader(val_ds, batch_size = batch_size)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5zVN1kHd43W"
   },
   "source": [
    "# Function for accuracy checking\n",
    "\n",
    "Don't change this code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OrxQRvfbahxH"
   },
   "source": [
    "def validate(model,testloader, device = \"cpu\"):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "  \n",
    "  return correct / total  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX-f8_6HrngE"
   },
   "source": [
    "# Implement CNN class for CIFAR10\n",
    "\n",
    "**In constructor**\n",
    "\n",
    "Define 2 - 3 convolutional layers \n",
    "\n",
    " https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "with corresponding in/out dimensions W_out = 1 + ((W_in - F + 2*P) / S)\n",
    "\n",
    "\n",
    "Also define max pooling : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "and fully connected layers: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "\n",
    "\n",
    "**In forward**\n",
    "\n",
    "Write code for forward pass.\n",
    "Remember that first dimension is the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3KX_n4_X0u8L"
   },
   "source": [
    "import torch.nn as nn\n",
    "nn.Conv2d?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_zLprG7kk-Q9"
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, class_nums = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16 * 3, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16 * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16 * 3, 32 * 3, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32 * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2))\n",
    "        \n",
    "        self.fc = nn.Linear(2400, class_nums)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        scores = self.layer1(batch)\n",
    "        scores = self.layer2(scores)\n",
    "        scores = scores.reshape(scores.size(0), -1)\n",
    "        scores = self.fc(scores)\n",
    "        return scores\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3DAyLmoryLp"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl3wwvZXsgfq"
   },
   "source": [
    "## First Activate tensorboard extension\n",
    "\n",
    "Use summaryWriter to create logs: https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter#torch.utils.tensorboard.writer.SummaryWriter\n",
    "\n",
    "Display loss and accuracy charts.\n",
    "\n",
    "You can cange log dir name."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vIhmoh_Fsfsa",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9HPW9T2tSRp"
   },
   "source": [
    "## Implement training loop\n",
    "\n",
    "- Create optimizer,\n",
    "- Save loss and accuracy values into tensorboard log\n",
    "- Use GPU to speedup training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qcJFI3hTOJlD"
   },
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TwoLayerCNN(10)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(15):\n",
    "  for step, (img_batch, labels_batch) in enumerate(train_loader):\n",
    "    img_batch = img_batch.to(device)\n",
    "    labels_batch = labels_batch.to(device)\n",
    "\n",
    "    output = model(img_batch)\n",
    "    loss = criterion(output, labels_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()   \n",
    "  \n",
    "  accuracy = validate(model, val_loader, device)\n",
    "\n",
    "  writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "  writer.add_scalar('Loss/train', loss.item(), epoch)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4tIFR5bwZFi"
   },
   "source": [
    "## Validat results on test dataset\n",
    "\n",
    "You must get accuracy above 0.65"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MM0pWYJlwibm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b6cb45ea-6778-4c63-feeb-ace562d94ff8"
   },
   "source": [
    "test_dataset = datasets.CIFAR10(\"content\",\n",
    "                           train=False,\n",
    "                           transform = dataset.transform, # Transforms stay the same\n",
    "                           download=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "\n",
    "accuracy = validate(model,test_loader,device)\n",
    "print(f\"Accuracy on test:{accuracy}\")\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Accuracy on test:0.6485\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIvyaeSVsIl0"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYMD7UT5BlAS"
   },
   "source": [
    "# Ideas for extra work\n",
    "\n",
    "---\n",
    "1. Evaluate the impact of the number and size of filters in convolutional layers on the accuracy.\n",
    "\n",
    "2. Evaluate the impact of the convolutional layers count on the accuracy.\n",
    "\n",
    "\n",
    "3. Visualization something ...\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}
